"""
æ··åˆæ–¹æ¡ˆï¼šä½¿ç”¨ Base æ¨¡å‹çš„ speaker_encoder + CustomVoice æ¨¡å‹ç”Ÿæˆ
"""
import torch
import soundfile as sf
import time
import json
from qwen_tts import Qwen3TTSModel
import os
import librosa
import numpy as np
from typing import List, Dict, Any


def load_valid_samples(jsonl_path: str) -> Dict[str, List[Any]]:
    """
    åŠ è½½ JSONL æ–‡ä»¶ï¼Œåªä¿ç•™æœ‰æ•ˆçš„æ ·æœ¬
    """
    samples = {
        'texts': [],
        'languages': [],
        'speakers': [],
        'instructs': [],
        'keys': [],
        'ref_texts': [],
        'ref_audios': []
    }
    
    skipped_count = 0
    
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            
            try:
                data = json.loads(line)
            except json.JSONDecodeError as e:
                print(f"âš ï¸  ç¬¬ {line_num} è¡Œ JSON è§£æå¤±è´¥: {e}")
                skipped_count += 1
                continue
            
            key = data.get('key', f'line_{line_num}')
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if "ref_audio" not in data or not data["ref_audio"]:
                print(f"âš ï¸  è·³è¿‡ {key}: ç¼ºå°‘ ref_audio")
                skipped_count += 1
                continue
            
            if not os.path.exists(data["ref_audio"]):
                print(f"âš ï¸  è·³è¿‡ {key}: ref_audio æ–‡ä»¶ä¸å­˜åœ¨: {data['ref_audio']}")
                skipped_count += 1
                continue
            
            if "ref_text" not in data or not data["ref_text"]:
                print(f"âš ï¸  è·³è¿‡ {key}: ç¼ºå°‘ ref_text")
                skipped_count += 1
                continue
            
            if "text" not in data or not data["text"]:
                print(f"âš ï¸  è·³è¿‡ {key}: ç¼ºå°‘ text")
                skipped_count += 1
                continue
            
            # æ·»åŠ æ ·æœ¬
            samples['texts'].append(data["text"])
            samples['languages'].append(data.get("language", "Chinese"))
            samples['speakers'].append(data.get("spk", ""))
            samples['instructs'].append(data.get("instruct", ""))
            samples['keys'].append(key)
            samples['ref_texts'].append(data["ref_text"])
            samples['ref_audios'].append(data["ref_audio"])
    
    print(f"\nâœ… ä» {jsonl_path} åŠ è½½äº† {len(samples['texts'])} ä¸ªæœ‰æ•ˆæ ·æœ¬")
    if skipped_count > 0:
        print(f"âš ï¸  è·³è¿‡äº† {skipped_count} ä¸ªæ— æ•ˆæ ·æœ¬\n")
    
    return samples


def extract_speaker_embeddings_with_base_model(
    base_model: Qwen3TTSModel,
    ref_audios: List[str],
) -> List[torch.Tensor]:
    """
    ä½¿ç”¨ Base æ¨¡å‹çš„ speaker_encoder æå– x-vectors
    
    Args:
        base_model: Base æ¨¡å‹å®ä¾‹
        ref_audios: å‚è€ƒéŸ³é¢‘è·¯å¾„åˆ—è¡¨
        
    Returns:
        List[torch.Tensor]: x-vector åˆ—è¡¨
    """
    print("ğŸ™ï¸  ä½¿ç”¨ Base æ¨¡å‹æå– speaker embeddings...")
    
    speaker_embeddings = []
    target_sr = base_model.model.speaker_encoder_sample_rate  # 24000
    
    for i, audio_path in enumerate(ref_audios):
        # åŠ è½½éŸ³é¢‘
        wav, sr = sf.read(audio_path)
        if wav.ndim > 1:
            wav = np.mean(wav, axis=-1)
        
        # é‡é‡‡æ ·åˆ° 24kHz
        if sr != target_sr:
            wav = librosa.resample(
                y=wav.astype(np.float32),
                orig_sr=int(sr),
                target_sr=target_sr
            )
        
        # æå– speaker embedding
        spk_emb = base_model.model.extract_speaker_embedding(
            audio=wav,
            sr=target_sr
        )
        speaker_embeddings.append(spk_emb)
        
        if (i + 1) % 10 == 0:
            print(f"  å·²å¤„ç† {i + 1}/{len(ref_audios)} ä¸ªéŸ³é¢‘")
    
    print(f"âœ… å®Œæˆ speaker embedding æå–\n")
    return speaker_embeddings


def extract_ref_codes_with_custom_model(
    custom_model: Qwen3TTSModel,
    ref_audios: List[str],
) -> List[torch.Tensor]:
    """
    ä½¿ç”¨ CustomVoice æ¨¡å‹çš„ speech_tokenizer æå– ref_codes
    
    Args:
        custom_model: CustomVoice æ¨¡å‹å®ä¾‹
        ref_audios: å‚è€ƒéŸ³é¢‘è·¯å¾„åˆ—è¡¨
        
    Returns:
        List[torch.Tensor]: ref_code åˆ—è¡¨
    """
    print("ğŸµ ä½¿ç”¨ CustomVoice æ¨¡å‹æå– ref_codes...")
    
    # æ‰¹é‡ç¼–ç 
    enc = custom_model.model.speech_tokenizer.encode(ref_audios)
    ref_codes = enc.audio_codes
    
    print(f"âœ… å®Œæˆ ref_code æå–\n")
    return ref_codes


def main():
    device = "cuda:0"
    
    # ========== æ­¥éª¤ 1: åŠ è½½ Base æ¨¡å‹ï¼ˆåªç”¨äºæå– speaker embeddingï¼‰==========
    print("=" * 80)
    print("æ­¥éª¤ 1: åŠ è½½ Base æ¨¡å‹ï¼ˆç”¨äºæå– speaker embeddingsï¼‰")
    print("=" * 80)
    
    base_model = Qwen3TTSModel.from_pretrained(
        "/data/Projects/Qwen3-TTS/pretrained_models/Qwen3-TTS-12Hz-1.7B-Base",
        device_map=device,
        dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
    )
    torch.cuda.synchronize()
    print(f"âœ… Base æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   æ¨¡å‹ç±»å‹: {base_model.model.tts_model_type}")
    print(f"   Tokenizer: {base_model.model.tokenizer_type}\n")
    
    # ========== æ­¥éª¤ 2: åŠ è½½ CustomVoice æ¨¡å‹ï¼ˆç”¨äºç”Ÿæˆï¼‰==========
    print("=" * 80)
    print("æ­¥éª¤ 2: åŠ è½½ CustomVoice æ¨¡å‹ï¼ˆç”¨äºç”Ÿæˆè¯­éŸ³ï¼‰")
    print("=" * 80)
    
    custom_model = Qwen3TTSModel.from_pretrained(
        "/data/Projects/Qwen3-TTS/exp/exp_l50/sft_lr2ef6_8spk_full-1.7B/checkpoint-epoch-3",
        device_map=device,
        dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
    )
    torch.cuda.synchronize()
    print(f"âœ… CustomVoice æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   æ¨¡å‹ç±»å‹: {custom_model.model.tts_model_type}")
    print(f"   Tokenizer: {custom_model.model.tokenizer_type}\n")
    
    # ========== æ­¥éª¤ 3: åŠ è½½æ•°æ® ==========
    print("=" * 80)
    print("æ­¥éª¤ 3: åŠ è½½æ•°æ®")
    print("=" * 80)
    
    jsonl_path = "/data/Projects/Qwen3-TTS/data/test/spoken.è‡ªç”±èŠå¤©é£æ ¼.prompt_True.jsonl"
    output_dir = "./output/hybrid-base-speaker-custom-generate"
    os.makedirs(output_dir, exist_ok=True)
    
    samples = load_valid_samples(jsonl_path)
    
    if len(samples['texts']) == 0:
        print("âŒ é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬å¯ä»¥å¤„ç†")
        return
    
    # ========== æ­¥éª¤ 4: æå– speaker embeddingsï¼ˆä½¿ç”¨ Base æ¨¡å‹ï¼‰==========
    print("=" * 80)
    print("æ­¥éª¤ 4: æå– speaker embeddings")
    print("=" * 80)
    
    speaker_embeddings = extract_speaker_embeddings_with_base_model(
        base_model=base_model,
        ref_audios=samples['ref_audios']
    )
    
    # ========== æ­¥éª¤ 5: æå– ref_codesï¼ˆä½¿ç”¨ CustomVoice æ¨¡å‹ï¼‰==========
    print("=" * 80)
    print("æ­¥éª¤ 5: æå– ref_codes")
    print("=" * 80)
    
    ref_codes = extract_ref_codes_with_custom_model(
        custom_model=custom_model,
        ref_audios=samples['ref_audios']
    )
    
    # ========== æ­¥éª¤ 6: æ„é€  voice_clone_prompt ==========
    print("=" * 80)
    print("æ­¥éª¤ 6: æ„é€  voice_clone_prompt")
    print("=" * 80)
    
    voice_clone_prompt = {
        "ref_spk_embedding": speaker_embeddings,
        "ref_code": ref_codes,
        "x_vector_only_mode": [False] * len(samples['texts']),  # ICL æ¨¡å¼
        "icl_mode": [True] * len(samples['texts']),
    }
    print(f"âœ… voice_clone_prompt æ„é€ å®Œæˆ\n")
    
    # ========== æ­¥éª¤ 7: æ„é€  ref_ids ==========
    print("=" * 80)
    print("æ­¥éª¤ 7: æ„é€  ref_ids")
    print("=" * 80)
    
    ref_ids = []
    for ref_text in samples['ref_texts']:
        ref_tok = custom_model._tokenize_texts(
            [custom_model._build_ref_text(ref_text)]
        )[0]
        ref_ids.append(ref_tok)
    print(f"âœ… ref_ids æ„é€ å®Œæˆ\n")
    
    # ========== æ­¥éª¤ 8: ç”Ÿæˆè¯­éŸ³ï¼ˆä½¿ç”¨ CustomVoice æ¨¡å‹ï¼‰==========
    print("=" * 80)
    print("æ­¥éª¤ 8: ç”Ÿæˆè¯­éŸ³")
    print("=" * 80)
    
    # æ„é€  input_ids
    input_ids = custom_model._tokenize_texts(
        [custom_model._build_assistant_text(t) for t in samples['texts']]
    )
    
    # æ„é€  instruct_ids
    instruct_ids = []
    for ins in samples['instructs']:
        if ins is None or ins == "":
            instruct_ids.append(None)
        else:
            instruct_ids.append(
                custom_model._tokenize_texts([custom_model._build_instruct_text(ins)])[0]
            )
    
    # ç”Ÿæˆå‚æ•°
    gen_kwargs = custom_model._merge_generate_kwargs(max_new_tokens=128)
    
    print("ğŸ¤ å¼€å§‹ç”Ÿæˆ...")
    t0 = time.time()
    
    talker_codes_list, _ = custom_model.model.generate(
        input_ids=input_ids,
        ref_ids=ref_ids,
        voice_clone_prompt=voice_clone_prompt,
        instruct_ids=instruct_ids,
        languages=samples['languages'],
        speakers=samples['speakers'],
        non_streaming_mode=True,
        **gen_kwargs,
    )
    
    torch.cuda.synchronize()
    t1 = time.time()
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {t1 - t0:.3f}s\n")
    
    # ========== æ­¥éª¤ 9: è§£ç å¹¶ä¿å­˜ ==========
    print("=" * 80)
    print("æ­¥éª¤ 9: è§£ç å¹¶ä¿å­˜éŸ³é¢‘")
    print("=" * 80)
    
    wavs, sr = custom_model.model.speech_tokenizer.decode(
        [{"audio_codes": c} for c in talker_codes_list]
    )
    
    print("ğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶...")
    for i, w in enumerate(wavs):
        key = samples['keys'][i]
        output_path = f"{output_dir}/{key}.wav"
        sf.write(output_path, w, sr)
        print(f"  âœ“ {output_path}")
    
    print(f"\nğŸ‰ å®Œæˆï¼å…±ç”Ÿæˆ {len(wavs)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # æ¸…ç†æ˜¾å­˜
    del base_model
    torch.cuda.empty_cache()
    print("\nâœ… å·²é‡Šæ”¾ Base æ¨¡å‹æ˜¾å­˜")


if __name__ == "__main__":
    main()
