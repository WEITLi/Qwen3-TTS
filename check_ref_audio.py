import torch
import soundfile as sf
import time
import json
from qwen_tts import Qwen3TTSModel
import os
from typing import List, Dict, Any


def load_valid_samples(jsonl_path: str) -> Dict[str, List[Any]]:
    """
    åŠ è½½ JSONL æ–‡ä»¶ï¼Œåªä¿ç•™æœ‰æ•ˆçš„æ ·æœ¬
    
    Returns:
        åŒ…å«æ‰€æœ‰å­—æ®µåˆ—è¡¨çš„å­—å…¸
    """
    samples = {
        'texts': [],
        'languages': [],
        'speakers': [],
        'instructs': [],
        'keys': [],
        'ref_texts': [],
        'ref_audios': []
    }
    
    skipped_count = 0
    
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            
            try:
                data = json.loads(line)
            except json.JSONDecodeError as e:
                print(f"âš ï¸  ç¬¬ {line_num} è¡Œ JSON è§£æå¤±è´¥: {e}")
                skipped_count += 1
                continue
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            key = data.get('key', f'line_{line_num}')
            
            # æ£€æŸ¥ ref_audio
            if "ref_audio" not in data or not data["ref_audio"]:
                print(f"âš ï¸  è·³è¿‡ {key}: ç¼ºå°‘ ref_audio")
                skipped_count += 1
                continue
            
            if not os.path.exists(data["ref_audio"]):
                print(f"âš ï¸  è·³è¿‡ {key}: ref_audio æ–‡ä»¶ä¸å­˜åœ¨: {data['ref_audio']}")
                skipped_count += 1
                continue
            
            # æ£€æŸ¥ ref_text
            if "ref_text" not in data or not data["ref_text"]:
                print(f"âš ï¸  è·³è¿‡ {key}: ç¼ºå°‘ ref_text")
                skipped_count += 1
                continue
            
            # æ£€æŸ¥ text
            if "text" not in data or not data["text"]:
                print(f"âš ï¸  è·³è¿‡ {key}: ç¼ºå°‘ text")
                skipped_count += 1
                continue
            
            # æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œæ·»åŠ æ ·æœ¬
            samples['texts'].append(data["text"])
            samples['languages'].append(data.get("language", "Chinese"))
            samples['speakers'].append(data.get("spk", ""))
            samples['instructs'].append(data.get("instruct", ""))
            samples['keys'].append(key)
            samples['ref_texts'].append(data["ref_text"])
            samples['ref_audios'].append(data["ref_audio"])
    
    print(f"\nâœ… ä» {jsonl_path} åŠ è½½äº† {len(samples['texts'])} ä¸ªæœ‰æ•ˆæ ·æœ¬")
    if skipped_count > 0:
        print(f"âš ï¸  è·³è¿‡äº† {skipped_count} ä¸ªæ— æ•ˆæ ·æœ¬\n")
    
    return samples


def main():
    device = "cuda:0"
    
    print("ğŸš€ åŠ è½½æ¨¡å‹...")
    tts = Qwen3TTSModel.from_pretrained(
        "/data/Projects/Qwen3-TTS/exp/exp_l50/sft_lr2ef6_8spk_full-1.7B/checkpoint-epoch-3",
        device_map=device,
        dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
    )
    torch.cuda.synchronize()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")

    jsonl_path = "/data/Projects/Qwen3-TTS/data/test/spoken.è‡ªç”±èŠå¤©é£æ ¼.prompt_True.jsonl"
    output_dir = "./output/custom-finetune-1.7B-plain_prompt_true_icl"
    os.makedirs(output_dir, exist_ok=True)

    # åŠ è½½æœ‰æ•ˆæ ·æœ¬
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    samples = load_valid_samples(jsonl_path)
    
    if len(samples['texts']) == 0:
        print("âŒ é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬å¯ä»¥å¤„ç†")
        return

    # ç”Ÿæˆè¯­éŸ³
    print("ğŸ¤ å¼€å§‹ç”Ÿæˆè¯­éŸ³...")
    t0 = time.time()
    wavs, sr = tts.generate_custom_voice_icl(
        text=samples['texts'],
        language=samples['languages'],
        speaker=samples['speakers'],
        instruct=samples['instructs'],
        max_new_tokens=128,
        ref_texts=samples['ref_texts'],    # â† ä½¿ç”¨ ref_textsï¼ˆå¤æ•°ï¼‰
        ref_audios=samples['ref_audios']   # â† ä½¿ç”¨ ref_audiosï¼ˆå¤æ•°ï¼‰
    )
    torch.cuda.synchronize()
    t1 = time.time()
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {t1 - t0:.3f}s\n")

    # ä¿å­˜
    print("ğŸ’¾ ä¿å­˜éŸ³é¢‘æ–‡ä»¶...")
    for i, w in enumerate(wavs):
        key = samples['keys'][i]
        output_path = f"{output_dir}/{key}.wav"
        sf.write(output_path, w, sr)
        print(f"  âœ“ {output_path}")
    
    print(f"\nğŸ‰ å®Œæˆï¼å…±ç”Ÿæˆ {len(wavs)} ä¸ªéŸ³é¢‘æ–‡ä»¶")


if __name__ == "__main__":
    main()
